{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ConversationalRetrievalChain\n",
        "\n",
        "*   conversation memory + RetrievalQAChain\n",
        "*   Allow for passing in conversation history which can be used for follow up questions.\n",
        "\n"
      ],
      "metadata": {
        "id": "M_gRn6O0BnyJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jN7JY_d4B6mf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \" \""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain openai chromadb tiktoken pypdf"
      ],
      "metadata": {
        "id": "TwP7ddc5B8MF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores import Chroma"
      ],
      "metadata": {
        "id": "hS_s_K_PB8Xi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationalRetrievalChain"
      ],
      "metadata": {
        "id": "O2e3fCsKB8bD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load document\n",
        "loader = PyPDFLoader(\"file_name.pdf\")\n",
        "documents = loader.load()\n",
        "# split the documents into chunks\n",
        "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "# select which embeddings we want to use\n",
        "# text-embedding-ada-002\n",
        "embeddings = OpenAIEmbeddings()\n",
        "# create the vectorestore to use as the index\n",
        "db = Chroma.from_documents(texts, embeddings)\n",
        "# expose this index in a retriever interface\n",
        "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":2}) # top 2 documnets\n",
        "# create a chain to answer questions\n",
        "qa = ConversationalRetrievalChain.from_llm(OpenAI(), retriever) # text-davinci-003 (GPT-3)\n",
        "chat_history = []"
      ],
      "metadata": {
        "id": "B0rs0Q3MCmry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"what is the overall summary of this conversation ?\"\n",
        "result = qa({\"question\": query, \"chat_history\": chat_history})"
      ],
      "metadata": {
        "id": "wftEyewAFWew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result[\"answer\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "gyyfwMV0Fa4Y",
        "outputId": "56bd8b66-e566-406f-b9b3-1e32c3362ecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' This conversation is about a data analytics platform that uses AI and machine learning to provide real-time insights. It has features like advanced data visualization tools, deep learning algorithms, and robust APIs for integration. It also uses parallel processing and distributed computing to handle large and intricate datasets.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = [(query, result[\"answer\"])]\n",
        "query = \"How is the agent attitude towards the customer?\"\n",
        "result = qa({\"question\": query, \"chat_history\": chat_history})"
      ],
      "metadata": {
        "id": "XczlreW9JT2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result[\"answer\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "98z_FChAJUG9",
        "outputId": "50547f09-8676-45e4-94a1-d90572f630ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' The agent has a helpful and patient attitude towards the customer.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_history = [(query, result[\"answer\"])]\n",
        "query = \"Is the agent was aggressive while talking to customer ?\"\n",
        "result = qa({\"question\": query, \"chat_history\": chat_history})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ib7xQnrCJtpl",
        "outputId": "9dd8dc91-44b1-4b13-d148-207a81eeff97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-text-davinci-003 in organization org-AW44Op4tGYDdSJyw3YHSTxNx on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result[\"answer\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AGvFpVe3J4gG",
        "outputId": "5ca9fa4f-c357-4a38-c0c4-cc05914b6661"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' No, the agent was not aggressive while talking to the customer.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}